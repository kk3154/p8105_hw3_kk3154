p8105\_hw3\_kk3154
================
Kristen King
10/20/2021

``` r
library(tidyverse)

knitr::opts_chunk$set(
  fid.width = 6, 
  fig.asp = 0.6,
  out.width = "90%")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis")


scale_color_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 - Exploring Instacart Data

Loading data from p8105.datasets:

``` r
library(p8105.datasets)
data("instacart")
```

**1.1** Describe the dataset, including size and structure of the data,
key variables, and giving illustrative examples of observations.

``` r
insta_df = instacart
```

The Instacart dataset contains 15 variables of data on 1384617 products
ordered in the instacart app. Each row is one product from each order,
and some identifying variables include order ID, customer ID, day/time
of order, and time since last order. Key variables for potential
analysis include product, aisle, and department IDs and names. For
example, observation \#9 in the dataset indicates customer number 79431
ordered Grated Pecorino Romano Cheese from the specialty cheeses aisle
in the dairy eggs department as part of their order number 36 at time
18:00 on the 6th day of the week.

**1.2** How many aisles are there, and which aisles are the most items
ordered from?

``` r
aisle_count = 
  insta_df %>% 
  mutate(aisle = factor(aisle)) %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) %>% 
  arrange(desc(n_items)) 

aisle_count %>%  
  head() %>% 
  knitr::kable()
```

| aisle                         | n\_items |
|:------------------------------|---------:|
| fresh vegetables              |   150609 |
| fresh fruits                  |   150473 |
| packaged vegetables fruits    |    78493 |
| yogurt                        |    55240 |
| packaged cheese               |    41699 |
| water seltzer sparkling water |    36617 |

There are 134 aisles in this dataset, and the fresh vegetables, fresh
fruits, packaged vegetables fruits, yogurt, packaged cheese, water
seltzer sparkling water aisles have the most items ordered from them.

**1.3** Make a plot that shows the number of items ordered in each
aisle, limiting this to aisles with more than 10000 items ordered.
Arrange aisles sensibly, and organize your plot so others can read it.

``` r
insta_df %>% 
  mutate(aisle = factor(aisle)) %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) %>% 
  filter(n_items > 10000) %>% 
  mutate(
    aisle = fct_reorder(aisle, n_items)
  ) %>% 
  ggplot(aes(x = aisle, y = n_items, xaxt = "")) + 
  geom_point(alpha = 0.3) + 
  scale_x_discrete(labels = NULL, breaks = NULL) + 
  labs(
    title = "Count of Items Ordered per Aisle",
    y = "Number of Items",
    x = "Aisle",
    caption = "Note: Only depicts aisles with >10,000 items ordered."
  )
```

<img src="p8105_hw3_kk3154_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" />

**1.4** Make a table showing the three most popular items in each of the
aisles “baking ingredients”, “dog food care”, and “packaged vegetables
fruits”. Include the number of times each item is ordered in your table.

**1.5** Make a table showing the mean hour of the day at which Pink Lady
Apples and Coffee Ice Cream are ordered on each day of the week; format
this table for human readers (i.e. produce a 2 x 7 table).

## Problem 2 - Cleaning and Exploring BRFSS Data

Loading the BRFSS data:

``` r
data("brfss_smart2010")
```

**2.1 Data cleaning:**

-   format the data to use appropriate variable names

-   focus on the “Overall Health” topic

-   include only responses from “Excellent” to “Poor”

-   organize responses as a factor taking levels ordered from “Poor” to
    “Excellent”

``` r
brfss_df = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, county = locationdesc) %>% 
  filter(topic == "Overall Health", response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) %>%
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), ordered=TRUE)) %>% 
  arrange(response)
```

**2.2** In 2002, which states were observed at 7 or more locations? What
about in 2010?

**2.3** Construct a dataset that is limited to `Excellent` responses,
and contains year, state, and a variable that averages the `data_value`
across locations within a state.

``` r
brfss_df_exc_only = brfss_df %>% 
  filter(response == "Excellent") %>% 
  select(year, state, data_value)
```

**2.4** Make a “spaghetti” plot of this average value over time within a
state (that is, make a plot showing a line for each state across years –
the `geom_line` geometry and `group` aesthetic will help).

**2.5** Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

## Problem 3 - Accelerometer data from a patient with congestive heart failure

35 days of accelerometer data where activity.\* variables are activity
counts for each minute of a 24-hour day starting at midnight.

**3.1** Loading, tidying, and wrangling data.

The final dataset includes all originally observed variables and values,
has useful variable names, includes a weekday vs. weekend variable, and
encodes data with reasonable variable classes.

``` r
acc_df = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(weekend = ifelse(day %in% c("Saturday", "Sunday"), 1, 0)) %>% 
  mutate(weekend = factor(weekend)) %>% 
  mutate(day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"), ordered=TRUE)) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute", 
    names_prefix = "activity_", 
    values_to = "activity_count"
  ) %>% 
  mutate(minute = as.numeric(minute))
```

    ## Rows: 35 Columns: 1443

    ## -- Column specification --------------------------------------------------------
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## i Use `spec()` to retrieve the full column specification for this data.
    ## i Specify the column types or set `show_col_types = FALSE` to quiet this message.

**3.2** Describing the resulting dataset (e.g. what variables exist, how
many observations, etc.):

The original accelerometer dataset included observations from 35 days
with 1443 columns, including 1440 activity count variables for each
minute in each 24 hour day. After tidying, the accelerometer dataset now
contains 50400 observations at the minute level that can be further
grouped by week, day, or day of the week. This tidy, longer datset now
contains only 6 variables: week, day\_id, day, weekend, minute,
activity\_count.

**3.3** Traditional analyses of accelerometer data focus on the total
activity over the day. Using your tidied dataset, aggregate across
minutes to create a total activity variable for each day, and create a
table showing these totals. Are any trends apparent?

**3.4** Accelerometer data allows the inspection activity over the
course of the day. Make a single-panel plot that shows the 24-hour
activity time courses for each day and use color to indicate day of the
week. Describe in words any patterns or conclusions you can make based
on this graph.
